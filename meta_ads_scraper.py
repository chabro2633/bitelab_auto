#!/usr/bin/env python3
"""
Meta Ad Library API ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸
- Meta Marketing API ì‚¬ìš© (ìŠ¤í¬ë˜í•‘ ëŒ€ì‹  ê³µì‹ API)
- GitHub Actionsì—ì„œ ì‹¤í–‰
- ê²°ê³¼ë¥¼ Vercel KVì— ì €ì¥
"""

import os
import sys
import json
import urllib.request
import urllib.parse
import urllib.error
import argparse
import logging
from datetime import datetime, timezone, timedelta

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜
VERCEL_KV_URL = os.getenv("KV_REST_API_URL")
VERCEL_KV_TOKEN = os.getenv("KV_REST_API_TOKEN")
META_ACCESS_TOKEN = os.getenv("META_ACCESS_TOKEN")

# Meta Ad Library API ì„¤ì •
META_API_VERSION = "v21.0"
META_API_BASE_URL = f"https://graph.facebook.com/{META_API_VERSION}"


def save_to_vercel_kv(key: str, data: dict, ttl: int = 3600):
    """Vercel KVì— ë°ì´í„° ì €ì¥ (TTL: 1ì‹œê°„)"""
    if not VERCEL_KV_URL or not VERCEL_KV_TOKEN:
        logger.warning("âš ï¸ Vercel KV í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False

    try:
        # Vercel KV REST API í˜•ì‹ìœ¼ë¡œ ìˆ˜ì •
        url = f"{VERCEL_KV_URL}/set/{key}"
        payload = json.dumps(data).encode('utf-8')

        # EX ì˜µì…˜ì„ URLì— í¬í•¨
        url_with_ttl = f"{url}?ex={ttl}"

        req = urllib.request.Request(
            url_with_ttl,
            data=payload,
            headers={
                'Authorization': f'Bearer {VERCEL_KV_TOKEN}',
                'Content-Type': 'application/json'
            },
            method='POST'
        )

        with urllib.request.urlopen(req, timeout=30) as response:
            if response.status == 200:
                logger.info(f"âœ… Vercel KV ì €ì¥ ì™„ë£Œ: {key}")
                return True
            else:
                logger.error(f"âŒ Vercel KV ì €ì¥ ì‹¤íŒ¨: HTTP {response.status}")
                return False
    except Exception as e:
        logger.error(f"âŒ Vercel KV ì €ì¥ ì˜¤ë¥˜: {e}")
        return False


def fetch_meta_ads(search_query: str, access_token: str, limit: int = 100, country: str = "KR"):
    """
    Meta Ad Library APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê´‘ê³  ë°ì´í„° ì¡°íšŒ

    API ë¬¸ì„œ: https://developers.facebook.com/docs/marketing-api/reference/ads_archive/
    """
    logger.info(f"ğŸ” ê²€ìƒ‰ì–´: {search_query}")
    logger.info(f"ğŸŒ êµ­ê°€: {country}")

    all_ads = []
    next_page_url = None
    page_count = 0
    max_pages = 10  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ì œí•œ

    # ê¸°ë³¸ API íŒŒë¼ë¯¸í„° (Ad Library API í˜•ì‹)
    # ad_reached_countriesëŠ” JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ì „ë‹¬
    base_params = {
        'access_token': access_token,
        'search_terms': search_query,
        'ad_reached_countries': f'["{country}"]',
        'ad_active_status': 'ACTIVE',
        'ad_type': 'ALL',
        'fields': ','.join([
            'id',
            'ad_creation_time',
            'ad_creative_bodies',
            'ad_creative_link_captions',
            'ad_creative_link_descriptions',
            'ad_creative_link_titles',
            'ad_delivery_start_time',
            'ad_snapshot_url',
            'bylines',
            'languages',
            'page_id',
            'page_name',
            'publisher_platforms'
        ]),
        'limit': str(limit)
    }

    # ì²« ë²ˆì§¸ ìš”ì²­ URL êµ¬ì„±
    query_string = urllib.parse.urlencode(base_params, safe='[]"')
    url = f"{META_API_BASE_URL}/ads_archive?{query_string}"

    while url and page_count < max_pages:
        page_count += 1
        logger.info(f"ğŸ“„ í˜ì´ì§€ {page_count} ì¡°íšŒ ì¤‘...")

        try:
            req = urllib.request.Request(url, method='GET')
            req.add_header('User-Agent', 'Mozilla/5.0')

            with urllib.request.urlopen(req, timeout=60) as response:
                data = json.loads(response.read().decode('utf-8'))

                ads = data.get('data', [])
                all_ads.extend(ads)
                logger.info(f"   â†’ {len(ads)}ê°œ ê´‘ê³  ìˆ˜ì§‘ (ì´ {len(all_ads)}ê°œ)")

                # ë‹¤ìŒ í˜ì´ì§€ URL
                paging = data.get('paging', {})
                url = paging.get('next')

                if not ads:
                    logger.info("ğŸ“­ ë” ì´ìƒ ê´‘ê³ ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break

        except urllib.error.HTTPError as e:
            error_body = e.read().decode('utf-8')
            logger.error(f"âŒ API ì˜¤ë¥˜ (HTTP {e.code}): {error_body}")

            # ì—ëŸ¬ ìƒì„¸ ë¶„ì„
            try:
                error_data = json.loads(error_body)
                error_msg = error_data.get('error', {}).get('message', '')
                error_code = error_data.get('error', {}).get('code', '')
                logger.error(f"   â†’ ì—ëŸ¬ ì½”ë“œ: {error_code}")
                logger.error(f"   â†’ ì—ëŸ¬ ë©”ì‹œì§€: {error_msg}")
            except:
                pass
            break

        except Exception as e:
            logger.error(f"âŒ ìš”ì²­ ì˜¤ë¥˜: {e}")
            break

    logger.info(f"âœ… ì´ {len(all_ads)}ê°œ ê´‘ê³  ìˆ˜ì§‘ ì™„ë£Œ ({page_count}í˜ì´ì§€)")
    return all_ads, page_count


def process_ads_data(ads: list):
    """ê´‘ê³  ë°ì´í„°ë¥¼ ê°€ê³µí•˜ì—¬ ë¯¸ë””ì–´ URL ë“± ì¶”ì¶œ"""
    processed_items = []

    for ad in ads:
        item = {
            'id': ad.get('id'),
            'page_name': ad.get('page_name'),
            'page_id': ad.get('page_id'),
            'ad_snapshot_url': ad.get('ad_snapshot_url'),
            'ad_creation_time': ad.get('ad_creation_time'),
            'ad_delivery_start_time': ad.get('ad_delivery_start_time'),
            'platforms': ad.get('publisher_platforms', []),
            'languages': ad.get('languages', []),
            'spend': ad.get('spend'),
            'impressions': ad.get('impressions'),
            'creative': {
                'bodies': ad.get('ad_creative_bodies', []),
                'link_titles': ad.get('ad_creative_link_titles', []),
                'link_descriptions': ad.get('ad_creative_link_descriptions', []),
                'link_captions': ad.get('ad_creative_link_captions', [])
            },
            'bylines': ad.get('bylines')
        }
        processed_items.append(item)

    return processed_items


def main():
    parser = argparse.ArgumentParser(description='Meta Ad Library API ì¡°íšŒ')
    parser.add_argument('--query', '-q', type=str, required=True, help='ê²€ìƒ‰ì–´')
    parser.add_argument('--request-id', '-r', type=str, required=True, help='ìš”ì²­ ID (ê²°ê³¼ ì €ì¥ìš©)')
    parser.add_argument('--limit', type=int, default=100, help='í˜ì´ì§€ë‹¹ ì¡°íšŒ ìˆ˜ (ê¸°ë³¸: 100)')
    parser.add_argument('--country', type=str, default='KR', help='êµ­ê°€ ì½”ë“œ (ê¸°ë³¸: KR)')
    args = parser.parse_args()

    # Access Token í™•ì¸
    access_token = META_ACCESS_TOKEN
    if not access_token:
        logger.error("âŒ META_ACCESS_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    logger.info("ğŸš€ Meta Ad Library API ì¡°íšŒ ì‹œì‘")
    logger.info(f"ğŸ“‹ ìš”ì²­ ID: {args.request_id}")

    KST = timezone(timedelta(hours=9))
    start_time = datetime.now(KST)

    try:
        # APIë¡œ ê´‘ê³  ë°ì´í„° ì¡°íšŒ
        ads, page_count = fetch_meta_ads(
            search_query=args.query,
            access_token=access_token,
            limit=args.limit,
            country=args.country
        )

        # ë°ì´í„° ê°€ê³µ
        processed_items = process_ads_data(ads)

        end_time = datetime.now(KST)

        result = {
            'success': True,
            'requestId': args.request_id,
            'searchQuery': args.query,
            'country': args.country,
            'totalItems': len(processed_items),
            'pageCount': page_count,
            'items': processed_items,
            'startTime': start_time.isoformat(),
            'endTime': end_time.isoformat(),
            'duration': (end_time - start_time).total_seconds()
        }

        # Vercel KVì— ì €ì¥
        kv_key = f"meta-ads:{args.request_id}"
        save_to_vercel_kv(kv_key, result, ttl=3600)  # 1ì‹œê°„ TTL

        # ê²°ê³¼ ì¶œë ¥
        logger.info("=" * 50)
        logger.info(f"âœ… ì¡°íšŒ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ê²€ìƒ‰ì–´: {args.query}")
        logger.info(f"ğŸ“¢ ìˆ˜ì§‘ëœ ê´‘ê³ : {len(processed_items)}ê°œ")
        logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {result['duration']:.1f}ì´ˆ")
        logger.info("=" * 50)

        # JSON ê²°ê³¼ íŒŒì¼ ì €ì¥ (GitHub Actions artifactìš©)
        result_file = f"meta_ads_result_{args.request_id}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, ensure_ascii=False, indent=2, fp=f)
        logger.info(f"ğŸ“ ê²°ê³¼ íŒŒì¼ ì €ì¥: {result_file}")

    except Exception as e:
        logger.error(f"âŒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

        error_result = {
            'success': False,
            'requestId': args.request_id,
            'searchQuery': args.query,
            'error': str(e),
            'endTime': datetime.now(KST).isoformat()
        }

        # ì—ëŸ¬ë„ KVì— ì €ì¥
        kv_key = f"meta-ads:{args.request_id}"
        save_to_vercel_kv(kv_key, error_result, ttl=3600)

        sys.exit(1)


if __name__ == "__main__":
    main()
