#!/usr/bin/env python3
"""
ê°œì„ ëœ Cigro ë°ì´í„° ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸
- í™˜ê²½ ë³€ìˆ˜ ì§€ì›
- ë¸Œëœë“œ ì„ íƒ ê¸°ëŠ¥
- ë‚ ì§œ ì„ íƒ ê¸°ëŠ¥
- ë°ì´í„° ìš°ì„ ìˆœìœ„ ë¡œì§ (ê¸°ì¡´ ë°ì´í„°ê°€ ë” ë§ìœ¼ë©´ ìœ ì§€)
- ì†ë„ ìµœì í™”
"""

import os
import sys
import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from playwright.sync_api import sync_playwright
from datetime import datetime, timedelta
import logging
import argparse

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
GOOGLE_SHEET_NAME = os.getenv("GOOGLE_SHEET_NAME", "Cigro Sales")
GOOGLE_CRED_FILE = os.getenv("GOOGLE_CRED_FILE", "google_sheet_credentials.json")
EMAIL = os.getenv("EMAIL", "tei.cha@bitelab.co.kr")
PASSWORD = os.getenv("PASSWORD", "qkfmsj123")

BRANDS = ["ë°”ë¥´ë„ˆ", "ë¦´ë¦¬ì´ë¸Œ", "ìƒ‰ë™ì„œìš¸", "ë¨¼ìŠ¬ë¦¬í”½", "ë³´í˜¸ë¦¬"]

def upload_to_google_sheets(df, sheet_name):
    """
    êµ¬ê¸€ ì‹œíŠ¸ì— ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ë” ë§ì€ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ êµì²´í•©ë‹ˆë‹¤.
    """
    try:
        scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
        creds = ServiceAccountCredentials.from_json_keyfile_name(GOOGLE_CRED_FILE, scope)
        client = gspread.authorize(creds)

        # ì‹œíŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        try:
            sheet = client.open(GOOGLE_SHEET_NAME).worksheet(sheet_name)
            logger.info(f"âœ… {sheet_name} ì‹œíŠ¸ ì°¾ê¸° ì™„ë£Œ")
        except gspread.exceptions.WorksheetNotFound:
            logger.info(f"âŒ {sheet_name} ì‹œíŠ¸ê°€ ì—†ìœ¼ë¯€ë¡œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            sheet = client.open(GOOGLE_SHEET_NAME).add_worksheet(title=sheet_name, rows="100", cols="20")

        # ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        existing_data = sheet.get_all_records(expected_headers=["date", "íŒë§¤ì²˜", "ì œí’ˆëª…", "ì˜µì…˜ëª…","íŒë§¤ëŸ‰","ê²°ì œê¸ˆì•¡","ì›ê°€","ìˆ˜ìˆ˜ë£Œ","ì»¬ëŸ¼1"])
        existing_df = pd.DataFrame(existing_data)

        # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸ ë° ì¶”ê°€
        if 'date' not in existing_df.columns:
            existing_df['date'] = ''
        if 'date' not in df.columns:
            df['date'] = ''

        # ìƒˆ ë°ì´í„°ì˜ ë‚ ì§œë“¤
        new_dates = df['date'].unique()
        
        for date in new_dates:
            existing_date_data = existing_df[existing_df['date'] == date]
            new_date_data = df[df['date'] == date]
            
            if existing_date_data.empty:
                # í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ì¶”ê°€
                sheet.append_rows(new_date_data.values.tolist(), value_input_option='RAW')
                logger.info(f"âœ… {sheet_name} ì‹œíŠ¸ì— {date} ë‚ ì§œ ë°ì´í„° ìƒˆë¡œ ì¶”ê°€ ì™„ë£Œ")
            else:
                # ë°ì´í„° ì–‘ ë¹„êµ (ê¸°ì¡´ ë°ì´í„°ê°€ ë” ë§ìœ¼ë©´ ìœ ì§€, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ìƒˆ ë°ì´í„°ë¡œ êµì²´)
                existing_count = len(existing_date_data)
                new_count = len(new_date_data)
                
                logger.info(f"ğŸ“Š {sheet_name} ì‹œíŠ¸ {date} ë‚ ì§œ ë°ì´í„° ë¹„êµ: ê¸°ì¡´ {existing_count}ê°œ vs ìƒˆ {new_count}ê°œ")
                
                if existing_count >= new_count:
                    logger.info(f"â„¹ï¸ {sheet_name} ì‹œíŠ¸ì˜ {date} ë‚ ì§œ ë°ì´í„°ê°€ ë” ë§ê±°ë‚˜ ê°™ìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°ì´í„°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.")
                else:
                    logger.info(f"ğŸ”„ {sheet_name} ì‹œíŠ¸ì˜ {date} ë‚ ì§œ ë°ì´í„°ë¥¼ ìƒˆ ë°ì´í„°ë¡œ êµì²´í•©ë‹ˆë‹¤.")
                    
                    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                    existing_indices = existing_df[existing_df['date'] == date].index.tolist()
                    sheet_row_numbers = [idx + 2 for idx in existing_indices]  # +2ëŠ” í—¤ë”ì™€ 0-based ì¸ë±ìŠ¤ ë•Œë¬¸
                    
                    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ê°œë³„ ì‚­ì œë¡œ ìˆ˜ì •)
                    if sheet_row_numbers:
                        # ë’¤ì—ì„œë¶€í„° ì‚­ì œí•˜ì—¬ ì¸ë±ìŠ¤ ë³€í™” ë°©ì§€
                        for row_num in sorted(sheet_row_numbers, reverse=True):
                            sheet.delete_rows(row_num)
                    
                    # ìƒˆ ë°ì´í„° ì¶”ê°€ (ë°°ì¹˜ ì—…ë¡œë“œë¡œ API í˜¸ì¶œ ìµœì†Œí™”)
                    if len(new_date_data) > 0:
                        sheet.append_rows(new_date_data.values.tolist(), value_input_option='RAW')
                    logger.info(f"âœ… {sheet_name} ì‹œíŠ¸ì˜ {date} ë‚ ì§œ ë°ì´í„° êµì²´ ì™„ë£Œ")
                    
    except Exception as e:
        logger.error(f"âŒ Google Sheets ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def extract_all_pages_data(page, selected_date, brand_name):
    """ëª¨ë“  í˜ì´ì§€ì˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    all_data = []
    headers = None
    current_page = 1
    expected_columns = 9  # ì˜ˆìƒë˜ëŠ” ì—´ ê°œìˆ˜ (ë‚ ì§œ í¬í•¨)

    while True:
        logger.info(f"ğŸ“„ {current_page}í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ ì¤‘...")

        # ì»¬ëŸ¼ ìš”ì†Œ ì°¾ê¸°
        columns = page.query_selector_all('div.sc-dkrFOg.cGhOUg')
        if not columns:
            logger.warning("âŒ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # í–‰ ë°ì´í„° ì¶”ì¶œ
        num_rows = len(columns[0].query_selector_all('div.sc-hLBbgP.jbaWzw'))
        for row_idx in range(num_rows):
            row_data = [selected_date]  # ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€
            for col in columns:
                cells = col.query_selector_all('div.sc-hLBbgP.jbaWzw')
                value = cells[row_idx].inner_text().strip() if row_idx < len(cells) else ''
                row_data.append(value)
            all_data.append(row_data)

        # í—¤ë” ì¶”ì¶œ (ì²« ë²ˆì§¸ í˜ì´ì§€ë§Œ)
        if headers is None:
            headers = ["date"] + [label.inner_text().strip() for label in page.query_selector_all('div.sc-gswNZR.gSJTZd > label')]
            
            # í—¤ë”ê°€ ë¹„ì–´ ìˆëŠ” ê²½ìš° ê¸°ë³¸ í—¤ë” ì¶”ê°€
            if not headers or len(headers) == 1:  # ë‹¨ì§€ "date"ë§Œ ìˆë‹¤ë©´
                headers = ["date"] + [f"ì»¬ëŸ¼{idx+1}" for idx in range(len(all_data[0]) - 1)]
                
            if len(headers) < len(all_data[0]):
                headers += [f"ì»¬ëŸ¼{idx+1}" for idx in range(len(all_data[0]) - len(headers))]

        # í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸ ë° í˜ì´ì§€ ì´ë™
        label_el = page.query_selector('label.text-cigro-page-number')
        page_text = label_el.inner_text().strip() if label_el else None
        if not page_text or f"{current_page} /" not in page_text:
            logger.warning("âŒ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        total_pages = int(page_text.split("/")[1].strip())
        if current_page >= total_pages:
            break

        # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        pagination_div = page.query_selector('div.w-20.flex.justify-between.items-center')
        svgs = pagination_div.query_selector_all('svg') if pagination_div else []
        if len(svgs) >= 3:
            svgs[2].click()
        else:
            break

        page.wait_for_timeout(1000)  # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        current_page += 1

    df = pd.DataFrame(all_data, columns=headers)
    
    # ì—´ ê°œìˆ˜ ê²€ì¦
    if len(df.columns) < expected_columns:
        logger.error(f"âŒ {brand_name} ë¸Œëœë“œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: ì˜ˆìƒ ì—´ ê°œìˆ˜ {expected_columns}ê°œ, ì‹¤ì œ {len(df.columns)}ê°œ")
        return None
    
    logger.info(f"âœ… {brand_name} ë¸Œëœë“œ ì´ {len(df)}ê°œ í–‰ì˜ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ (ì—´ ê°œìˆ˜: {len(df.columns)}ê°œ)")
    return df

def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ìˆ˜ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(description='Cigro ë°ì´í„° ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--date', type=str, help='ìŠ¤í¬ë˜í•‘í•  ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)')
    parser.add_argument('--brands', type=str, nargs='+', help='ìŠ¤í¬ë˜í•‘í•  ë¸Œëœë“œ ëª©ë¡ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)')
    parser.add_argument('--headless', action='store_true', default=True, help='í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰')
    return parser.parse_args()

def main():
    args = parse_arguments()
    
    logger.info("ğŸš€ Cigro ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    
    # ë‚ ì§œ ì„¤ì •
    if args.date:
        try:
            selected_date = datetime.strptime(args.date, "%Y-%m-%d").strftime("%Y-%m-%d")
            logger.info(f"ğŸ“… ì§€ì •ëœ ë‚ ì§œë¡œ ìŠ¤í¬ë˜í•‘: {selected_date}")
        except ValueError:
            logger.error(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {args.date}. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
    else:
        yesterday = datetime.now() - timedelta(1)
        selected_date = yesterday.strftime("%Y-%m-%d")
        logger.info(f"ğŸ“… ì–´ì œ ë‚ ì§œë¡œ ìŠ¤í¬ë˜í•‘: {selected_date}")
    
    # ë¸Œëœë“œ ì„¤ì •
    if args.brands:
        selected_brands = args.brands
        logger.info(f"ğŸ“‹ ì„ íƒëœ ë¸Œëœë“œ: {', '.join(selected_brands)}")
    else:
        selected_brands = BRANDS
        logger.info(f"ğŸ“‹ ëª¨ë“  ë¸Œëœë“œ ìŠ¤í¬ë˜í•‘: {', '.join(selected_brands)}")

    with sync_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹¤í–‰ ì„¤ì •
        browser_args = [
            '--no-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',
            '--disable-web-security',
            '--disable-features=VizDisplayCompositor'
        ]
        
        browser = p.chromium.launch(
            headless=args.headless,
            args=browser_args
        )

        try:
            if os.path.exists("auth.json"):
                logger.info("ğŸ” ê¸°ì¡´ ë¡œê·¸ì¸ ì„¸ì…˜ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
                context = browser.new_context(storage_state="auth.json")
            else:
                logger.info("ğŸ§­ ì„¸ì…˜ ì—†ìŒ âœ ìˆ˜ë™ ë¡œê·¸ì¸ ì‹œì‘")
                context = browser.new_context()
                page = context.new_page()
                page.goto("https://app.cigro.io")
                logger.info("ğŸ“ ë¡œê·¸ì¸ ìë™í™” ì¤‘...")
                
                # ì´ë©”ì¼, ë¹„ë°€ë²ˆí˜¸ ìë™ ì…ë ¥
                page.fill('input.bubble-element.Input.cnaNaCaE0.a1746627658297x1166[type="email"]', EMAIL)
                page.fill('input[type="password"]', PASSWORD)
                
                # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
                page.click('div.clickable-element.bubble-element.Group.cnaNaCaF0.bubble-r-container')
                page.wait_for_timeout(5000)

                logger.info("ğŸ” ë¡œê·¸ì¸ ì™„ë£Œ í›„ ì„¸ì…˜ ì €ì¥ ì¤‘...")
                context.storage_state(path="auth.json")

            # ê° ë¸Œëœë“œë³„ë¡œ ë°ì´í„° ì¶”ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            successful_brands = []
            failed_brands = []
            
            for brand in selected_brands:
                logger.info(f"ğŸ” {brand} ë°ì´í„° ì¶”ì¶œ ì¤‘...")
                max_retries = 3
                success = False
                
                for attempt in range(max_retries):
                    try:
                        target_url = f"https://app.cigro.io/?menu=analysis&tab=product&group_by=option&brand_name={brand}&start_date={selected_date}&end_date={selected_date}"

                        page = context.new_page()
                        page.goto(target_url)
                        page.wait_for_timeout(5000)  # í…Œì´ë¸” ë¡œë”© ëŒ€ê¸°

                        df = extract_all_pages_data(page, selected_date, brand)
                        
                        if df is not None and not df.empty:
                            upload_to_google_sheets(df, brand)
                            successful_brands.append(brand)
                            success = True
                            logger.info(f"âœ… {brand} ë¸Œëœë“œ ì²˜ë¦¬ ì™„ë£Œ")
                            break
                        else:
                            logger.warning(f"âš ï¸ {brand} ë¸Œëœë“œ ì‹œë„ {attempt + 1}/{max_retries}: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                            if attempt < max_retries - 1:
                                logger.info(f"ğŸ”„ {brand} ë¸Œëœë“œ ì¬ì‹œë„ ì¤‘... ({attempt + 2}/{max_retries})")
                                page.wait_for_timeout(3000)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                        
                    except Exception as e:
                        logger.error(f"âŒ {brand} ë¸Œëœë“œ ì‹œë„ {attempt + 1}/{max_retries} ì¤‘ ì˜¤ë¥˜: {e}")
                        if attempt < max_retries - 1:
                            logger.info(f"ğŸ”„ {brand} ë¸Œëœë“œ ì¬ì‹œë„ ì¤‘... ({attempt + 2}/{max_retries})")
                            page.wait_for_timeout(3000)
                    finally:
                        try:
                            page.close()
                        except:
                            pass
                
                if not success:
                    failed_brands.append(brand)
                    logger.error(f"âŒ {brand} ë¸Œëœë“œ ìµœì¢… ì‹¤íŒ¨ (ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼)")

            # ìµœì¢… ê²°ê³¼ ìš”ì•½
            logger.info("=" * 50)
            logger.info("ğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìš”ì•½")
            logger.info(f"âœ… ì„±ê³µí•œ ë¸Œëœë“œ: {', '.join(successful_brands) if successful_brands else 'ì—†ìŒ'}")
            if failed_brands:
                logger.error(f"âŒ ì‹¤íŒ¨í•œ ë¸Œëœë“œ: {', '.join(failed_brands)}")
            logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {len(successful_brands)}/{len(selected_brands)} ({len(successful_brands)/len(selected_brands)*100:.1f}%)")
            logger.info("=" * 50)
            
            if successful_brands:
                logger.info("ğŸ‰ ìŠ¤í¬ë˜í•‘ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                logger.error("âŒ ëª¨ë“  ë¸Œëœë“œ ìŠ¤í¬ë˜í•‘ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            browser.close()

if __name__ == "__main__":
    main()