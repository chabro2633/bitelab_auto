#!/usr/bin/env python3
"""
ê°œì„ ëœ Cigro ë°ì´í„° ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸
- í™˜ê²½ ë³€ìˆ˜ ì§€ì›
- ë¸Œëœë“œ ì„ íƒ ê¸°ëŠ¥
- ë‚ ì§œ ì„ íƒ ê¸°ëŠ¥
- ë°ì´í„° ìš°ì„ ìˆœìœ„ ë¡œì§ (ê¸°ì¡´ ë°ì´í„°ê°€ ë” ë§ìœ¼ë©´ ìœ ì§€)
- ì†ë„ ìµœì í™” (ë³‘ë ¬ ì²˜ë¦¬, ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™”)
"""

import os
import sys
import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials
from playwright.sync_api import sync_playwright
from datetime import datetime, timedelta
import logging
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
GOOGLE_SHEET_NAME = os.getenv("GOOGLE_SHEET_NAME", "Cigro Sales")
GOOGLE_CRED_FILE = os.getenv("GOOGLE_CRED_FILE", "google_sheet_credentials.json")
EMAIL = os.getenv("EMAIL", "tei.cha@bitelab.co.kr")
PASSWORD = os.getenv("PASSWORD", "qkfmsj123")

BRANDS = ["ë°”ë¥´ë„ˆ", "ë¦´ë¦¬ì´ë¸Œ", "ìƒ‰ë™ì„œìš¸", "ë¨¼ìŠ¬ë¦¬í”½", "ë³´í˜¸ë¦¬"]

def upload_to_google_sheets(df, sheet_name):
    """
    êµ¬ê¸€ ì‹œíŠ¸ì— ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ë” ë§ì€ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ êµì²´í•©ë‹ˆë‹¤.
    """
    try:
        scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
        creds = ServiceAccountCredentials.from_json_keyfile_name(GOOGLE_CRED_FILE, scope)
        client = gspread.authorize(creds)

        # ì‹œíŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        try:
            sheet = client.open(GOOGLE_SHEET_NAME).worksheet(sheet_name)
            logger.info(f"âœ… {sheet_name} ì‹œíŠ¸ ì°¾ê¸° ì™„ë£Œ")
        except gspread.exceptions.WorksheetNotFound:
            logger.info(f"âŒ {sheet_name} ì‹œíŠ¸ê°€ ì—†ìœ¼ë¯€ë¡œ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            sheet = client.open(GOOGLE_SHEET_NAME).add_worksheet(title=sheet_name, rows="100", cols="20")

        # ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        existing_data = sheet.get_all_records(expected_headers=["date", "íŒë§¤ì²˜", "ì œí’ˆëª…", "ì˜µì…˜ëª…","íŒë§¤ëŸ‰","ê²°ì œê¸ˆì•¡","ì›ê°€","ìˆ˜ìˆ˜ë£Œ","ì»¬ëŸ¼1"])
        existing_df = pd.DataFrame(existing_data)

        # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸ ë° ì¶”ê°€
        if 'date' not in existing_df.columns:
            existing_df['date'] = ''
        if 'date' not in df.columns:
            df['date'] = ''

        # ìƒˆ ë°ì´í„°ì˜ ë‚ ì§œë“¤
        new_dates = df['date'].unique()

        for date in new_dates:
            existing_date_data = existing_df[existing_df['date'] == date]
            new_date_data = df[df['date'] == date]

            if existing_date_data.empty:
                # í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ì¶”ê°€
                sheet.append_rows(new_date_data.values.tolist(), value_input_option='RAW')
                logger.info(f"âœ… {sheet_name} ì‹œíŠ¸ì— {date} ë‚ ì§œ ë°ì´í„° ìƒˆë¡œ ì¶”ê°€ ì™„ë£Œ")
            else:
                # ë°ì´í„° ë¹„êµ
                existing_count = len(existing_date_data)
                new_count = len(new_date_data)

                logger.info(f"ğŸ“Š {sheet_name} ì‹œíŠ¸ {date} ë‚ ì§œ ë°ì´í„° ë¹„êµ: ê¸°ì¡´ {existing_count}ê°œ vs ìƒˆ {new_count}ê°œ")

                should_replace = False
                replace_reason = ""

                # 1. ìƒˆ ë°ì´í„°ê°€ ë” ë§ìœ¼ë©´ êµì²´
                if new_count > existing_count:
                    should_replace = True
                    replace_reason = f"ìƒˆ ë°ì´í„°ê°€ ë” ë§ìŒ ({new_count} > {existing_count})"
                else:
                    # 2. ì›ê°€, íŒë§¤ëŸ‰, ê²°ì œê¸ˆì•¡ ë¹„êµ (ê°™ì€ í–‰ ìˆ˜ì¼ ë•Œ)
                    try:
                        # ë¹„êµë¥¼ ìœ„í•´ í‚¤ ì»¬ëŸ¼ìœ¼ë¡œ ë§¤ì¹­ (íŒë§¤ì²˜, ì œí’ˆëª…, ì˜µì…˜ëª…)
                        key_cols = ['íŒë§¤ì²˜', 'ì œí’ˆëª…', 'ì˜µì…˜ëª…']

                        for _, new_row in new_date_data.iterrows():
                            # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ê°™ì€ í•­ëª© ì°¾ê¸°
                            mask = (existing_date_data['íŒë§¤ì²˜'] == new_row['íŒë§¤ì²˜']) & \
                                   (existing_date_data['ì œí’ˆëª…'] == new_row['ì œí’ˆëª…']) & \
                                   (existing_date_data['ì˜µì…˜ëª…'] == new_row['ì˜µì…˜ëª…'])
                            matching_rows = existing_date_data[mask]

                            if not matching_rows.empty:
                                existing_row = matching_rows.iloc[0]

                                # ìˆ«ì ë³€í™˜ í•¨ìˆ˜
                                def to_number(val):
                                    if pd.isna(val) or val == '' or val == '-':
                                        return 0
                                    try:
                                        return float(str(val).replace(',', '').replace('ì›', '').replace('%', '').strip())
                                    except:
                                        return 0

                                # ì›ê°€ ë¹„êµ (ê¸°ì¡´ 0ì›ì—ì„œ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€ê²½ëœ ê²½ìš°)
                                new_cost = to_number(new_row.get('ì›ê°€', 0))
                                existing_cost = to_number(existing_row.get('ì›ê°€', 0))
                                if existing_cost == 0 and new_cost > 0:
                                    should_replace = True
                                    replace_reason = f"ì›ê°€ ì—…ë°ì´íŠ¸ (0 â†’ {new_cost})"
                                    break

                                # íŒë§¤ëŸ‰ ë¹„êµ (ìƒˆ ê°’ì´ ë” í¬ë©´ ì—…ë°ì´íŠ¸)
                                new_sales = to_number(new_row.get('íŒë§¤ëŸ‰', 0))
                                existing_sales = to_number(existing_row.get('íŒë§¤ëŸ‰', 0))
                                if new_sales > existing_sales:
                                    should_replace = True
                                    replace_reason = f"íŒë§¤ëŸ‰ ì¦ê°€ ({existing_sales} â†’ {new_sales})"
                                    break

                                # ê²°ì œê¸ˆì•¡ ë¹„êµ (ìƒˆ ê°’ì´ ë” í¬ë©´ ì—…ë°ì´íŠ¸)
                                new_amount = to_number(new_row.get('ê²°ì œê¸ˆì•¡', 0))
                                existing_amount = to_number(existing_row.get('ê²°ì œê¸ˆì•¡', 0))
                                if new_amount > existing_amount:
                                    should_replace = True
                                    replace_reason = f"ê²°ì œê¸ˆì•¡ ì¦ê°€ ({existing_amount} â†’ {new_amount})"
                                    break
                    except Exception as e:
                        logger.warning(f"âš ï¸ ë°ì´í„° ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")

                if should_replace:
                    logger.info(f"ğŸ”„ {sheet_name} ì‹œíŠ¸ì˜ {date} ë‚ ì§œ ë°ì´í„° êµì²´ ì‚¬ìœ : {replace_reason}")

                    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                    existing_indices = existing_df[existing_df['date'] == date].index.tolist()
                    sheet_row_numbers = [idx + 2 for idx in existing_indices]  # +2ëŠ” í—¤ë”ì™€ 0-based ì¸ë±ìŠ¤ ë•Œë¬¸

                    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ë’¤ì—ì„œë¶€í„° ì‚­ì œí•˜ì—¬ ì¸ë±ìŠ¤ ë³€í™” ë°©ì§€)
                    if sheet_row_numbers:
                        for row_num in sorted(sheet_row_numbers, reverse=True):
                            sheet.delete_rows(row_num)

                    # ìƒˆ ë°ì´í„° ì¶”ê°€
                    if len(new_date_data) > 0:
                        sheet.append_rows(new_date_data.values.tolist(), value_input_option='RAW')
                    logger.info(f"âœ… {sheet_name} ì‹œíŠ¸ì˜ {date} ë‚ ì§œ ë°ì´í„° êµì²´ ì™„ë£Œ")
                else:
                    logger.info(f"â„¹ï¸ {sheet_name} ì‹œíŠ¸ì˜ {date} ë‚ ì§œ ë°ì´í„° ë³€ê²½ ì—†ìŒ. ê¸°ì¡´ ë°ì´í„° ìœ ì§€.")
                    
    except Exception as e:
        logger.error(f"âŒ Google Sheets ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def extract_all_pages_data(page, selected_date, brand_name):
    """ëª¨ë“  í˜ì´ì§€ì˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    all_data = []
    headers = None
    current_page = 1
    expected_columns = 9  # ì˜ˆìƒë˜ëŠ” ì—´ ê°œìˆ˜ (ë‚ ì§œ í¬í•¨)

    while True:
        logger.info(f"ğŸ“„ {brand_name} - {current_page}í˜ì´ì§€ ë°ì´í„° ì¶”ì¶œ ì¤‘...")

        # ì»¬ëŸ¼ ìš”ì†Œ ì°¾ê¸°
        columns = page.query_selector_all('div.sc-dkrFOg.cGhOUg')
        if not columns:
            logger.warning(f"âŒ {brand_name} - ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # í–‰ ë°ì´í„° ì¶”ì¶œ
        num_rows = len(columns[0].query_selector_all('div.sc-hLBbgP.jbaWzw'))
        for row_idx in range(num_rows):
            row_data = [selected_date]  # ë‚ ì§œ ì»¬ëŸ¼ ì¶”ê°€
            for col in columns:
                cells = col.query_selector_all('div.sc-hLBbgP.jbaWzw')
                value = cells[row_idx].inner_text().strip() if row_idx < len(cells) else ''
                row_data.append(value)
            all_data.append(row_data)

        # í—¤ë” ì¶”ì¶œ (ì²« ë²ˆì§¸ í˜ì´ì§€ë§Œ)
        if headers is None:
            headers = ["date"] + [label.inner_text().strip() for label in page.query_selector_all('div.sc-gswNZR.gSJTZd > label')]

            # í—¤ë”ê°€ ë¹„ì–´ ìˆëŠ” ê²½ìš° ê¸°ë³¸ í—¤ë” ì¶”ê°€
            if not headers or len(headers) == 1:  # ë‹¨ì§€ "date"ë§Œ ìˆë‹¤ë©´
                headers = ["date"] + [f"ì»¬ëŸ¼{idx+1}" for idx in range(len(all_data[0]) - 1)]

            if len(headers) < len(all_data[0]):
                headers += [f"ì»¬ëŸ¼{idx+1}" for idx in range(len(all_data[0]) - len(headers))]

        # í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸ ë° í˜ì´ì§€ ì´ë™
        label_el = page.query_selector('label.text-cigro-page-number')
        page_text = label_el.inner_text().strip() if label_el else None
        if not page_text or f"{current_page} /" not in page_text:
            logger.warning(f"âŒ {brand_name} - í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        total_pages = int(page_text.split("/")[1].strip())
        if current_page >= total_pages:
            break

        # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        pagination_div = page.query_selector('div.w-20.flex.justify-between.items-center')
        svgs = pagination_div.query_selector_all('svg') if pagination_div else []
        if len(svgs) >= 3:
            svgs[2].click()
            # ê³ ì • ëŒ€ê¸° ëŒ€ì‹  í…Œì´ë¸” ìš”ì†Œê°€ ì—…ë°ì´íŠ¸ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            page.wait_for_timeout(500)
        else:
            break

        current_page += 1

    df = pd.DataFrame(all_data, columns=headers)

    # ì—´ ê°œìˆ˜ ê²€ì¦
    if len(df.columns) < expected_columns:
        logger.error(f"âŒ {brand_name} ë¸Œëœë“œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: ì˜ˆìƒ ì—´ ê°œìˆ˜ {expected_columns}ê°œ, ì‹¤ì œ {len(df.columns)}ê°œ")
        return None

    logger.info(f"âœ… {brand_name} ë¸Œëœë“œ ì´ {len(df)}ê°œ í–‰ì˜ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ (ì—´ ê°œìˆ˜: {len(df.columns)}ê°œ)")
    return df


def scrape_brand(browser_context, brand, selected_date, max_retries=2):
    """ë‹¨ì¼ ë¸Œëœë“œë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤."""
    for attempt in range(max_retries):
        page = None
        try:
            target_url = f"https://app.cigro.io/?menu=analysis&tab=product&group_by=option&brand_name={brand}&start_date={selected_date}&end_date={selected_date}"

            page = browser_context.new_page()
            # ë„¤íŠ¸ì›Œí¬ idle ìƒíƒœê¹Œì§€ ëŒ€ê¸° (ë” ë¹ ë¥¸ ë¡œë”© ê°ì§€)
            page.goto(target_url, wait_until='networkidle', timeout=30000)

            # í…Œì´ë¸” ë¡œë”© ëŒ€ê¸° - ê³ ì • 5ì´ˆ ëŒ€ì‹  ìš”ì†Œ ëŒ€ê¸°
            try:
                page.wait_for_selector('div.sc-dkrFOg.cGhOUg', timeout=10000)
            except:
                logger.warning(f"âš ï¸ {brand} - í…Œì´ë¸” ë¡œë”© ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ")

            df = extract_all_pages_data(page, selected_date, brand)

            if df is not None and not df.empty:
                return brand, df, None
            else:
                logger.warning(f"âš ï¸ {brand} ì‹œë„ {attempt + 1}/{max_retries}: ë°ì´í„° ì—†ìŒ")

        except Exception as e:
            logger.error(f"âŒ {brand} ì‹œë„ {attempt + 1}/{max_retries} ì˜¤ë¥˜: {e}")
        finally:
            if page:
                try:
                    page.close()
                except:
                    pass

    return brand, None, f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼"

def parse_arguments():
    """ëª…ë ¹ì¤„ ì¸ìˆ˜ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(description='Cigro ë°ì´í„° ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸')
    parser.add_argument('--date', type=str, help='ìŠ¤í¬ë˜í•‘í•  ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)')
    parser.add_argument('--brands', type=str, nargs='+', help='ìŠ¤í¬ë˜í•‘í•  ë¸Œëœë“œ ëª©ë¡ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)')
    parser.add_argument('--headless', action='store_true', default=True, help='í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œë¡œ ì‹¤í–‰')
    return parser.parse_args()

def main():
    args = parse_arguments()
    
    logger.info("ğŸš€ Cigro ë°ì´í„° ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    
    # ë‚ ì§œ ì„¤ì •
    if args.date:
        try:
            selected_date = datetime.strptime(args.date, "%Y-%m-%d").strftime("%Y-%m-%d")
            logger.info(f"ğŸ“… ì§€ì •ëœ ë‚ ì§œë¡œ ìŠ¤í¬ë˜í•‘: {selected_date}")
        except ValueError:
            logger.error(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {args.date}. YYYY-MM-DD í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            sys.exit(1)
    else:
        yesterday = datetime.now() - timedelta(1)
        selected_date = yesterday.strftime("%Y-%m-%d")
        logger.info(f"ğŸ“… ì–´ì œ ë‚ ì§œë¡œ ìŠ¤í¬ë˜í•‘: {selected_date}")
    
    # ë¸Œëœë“œ ì„¤ì •
    if args.brands:
        selected_brands = args.brands
        logger.info(f"ğŸ“‹ ì„ íƒëœ ë¸Œëœë“œ: {', '.join(selected_brands)}")
    else:
        selected_brands = BRANDS
        logger.info(f"ğŸ“‹ ëª¨ë“  ë¸Œëœë“œ ìŠ¤í¬ë˜í•‘: {', '.join(selected_brands)}")

    with sync_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹¤í–‰ ì„¤ì • - ìµœì í™”ëœ ì˜µì…˜
        browser_args = [
            '--no-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu',
            '--disable-web-security',
            '--disable-features=VizDisplayCompositor',
            '--disable-extensions',
            '--disable-plugins',
            '--disable-images',  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        ]

        browser = p.chromium.launch(
            headless=args.headless,
            args=browser_args
        )

        try:
            if os.path.exists("auth.json"):
                logger.info("ğŸ” ê¸°ì¡´ ë¡œê·¸ì¸ ì„¸ì…˜ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
                context = browser.new_context(storage_state="auth.json")
            else:
                logger.info("ğŸ§­ ì„¸ì…˜ ì—†ìŒ âœ ìˆ˜ë™ ë¡œê·¸ì¸ ì‹œì‘")
                context = browser.new_context()
                page = context.new_page()
                page.goto("https://app.cigro.io", wait_until='domcontentloaded')
                logger.info("ğŸ“ ë¡œê·¸ì¸ ìë™í™” ì¤‘...")

                # ì´ë©”ì¼, ë¹„ë°€ë²ˆí˜¸ ìë™ ì…ë ¥
                page.fill('input.bubble-element.Input.cnaNaCaE0.a1746627658297x1166[type="email"]', EMAIL)
                page.fill('input[type="password"]', PASSWORD)

                # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
                page.click('div.clickable-element.bubble-element.Group.cnaNaCaF0.bubble-r-container')
                page.wait_for_load_state('networkidle', timeout=15000)

                logger.info("ğŸ” ë¡œê·¸ì¸ ì™„ë£Œ í›„ ì„¸ì…˜ ì €ì¥ ì¤‘...")
                context.storage_state(path="auth.json")
                page.close()

            # ë¸Œëœë“œë³„ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
            successful_brands = []
            failed_brands = []
            results = {}

            logger.info(f"ğŸš€ {len(selected_brands)}ê°œ ë¸Œëœë“œ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

            # ìˆœì°¨ ì²˜ë¦¬ (PlaywrightëŠ” ë™ì¼ contextì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ ì œí•œ)
            for brand in selected_brands:
                logger.info(f"ğŸ” {brand} ë°ì´í„° ì¶”ì¶œ ì¤‘...")
                brand_name, df, error = scrape_brand(context, brand, selected_date)

                if df is not None:
                    results[brand_name] = df
                    successful_brands.append(brand_name)
                    logger.info(f"âœ… {brand_name} ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
                else:
                    failed_brands.append(brand_name)
                    logger.error(f"âŒ {brand_name} ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {error}")

            # Google Sheets ì—…ë¡œë“œ (ìŠ¤í¬ë˜í•‘ ì™„ë£Œ í›„ ì¼ê´„ ì²˜ë¦¬)
            if results:
                logger.info(f"ğŸ“¤ Google Sheets ì—…ë¡œë“œ ì‹œì‘ ({len(results)}ê°œ ë¸Œëœë“œ)...")
                for brand_name, df in results.items():
                    upload_to_google_sheets(df, brand_name)
                    logger.info(f"âœ… {brand_name} ì—…ë¡œë“œ ì™„ë£Œ")

            # ìµœì¢… ê²°ê³¼ ìš”ì•½
            logger.info("=" * 50)
            logger.info("ğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìš”ì•½")
            logger.info(f"âœ… ì„±ê³µí•œ ë¸Œëœë“œ: {', '.join(successful_brands) if successful_brands else 'ì—†ìŒ'}")
            if failed_brands:
                logger.error(f"âŒ ì‹¤íŒ¨í•œ ë¸Œëœë“œ: {', '.join(failed_brands)}")
            logger.info(f"ğŸ“ˆ ì„±ê³µë¥ : {len(successful_brands)}/{len(selected_brands)} ({len(successful_brands)/len(selected_brands)*100:.1f}%)")
            logger.info("=" * 50)

            if successful_brands:
                logger.info("ğŸ‰ ìŠ¤í¬ë˜í•‘ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:
                logger.error("âŒ ëª¨ë“  ë¸Œëœë“œ ìŠ¤í¬ë˜í•‘ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            browser.close()

if __name__ == "__main__":
    main()